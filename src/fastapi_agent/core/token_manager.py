"""Token management for message history with automatic summarization."""

from typing import Any

import tiktoken

from fastapi_agent.core.llm_client import LLMClient
from fastapi_agent.schemas.message import Message


class TokenManager:
    """Manages token counting and message history summarization.

    Features:
    - Accurate token counting using tiktoken (cl100k_base encoder)
    - Automatic message history summarization when rounds exceed threshold
    - Preserves user messages while summarizing agent execution rounds
    - Extracts core memory for context continuity
    - Fallback to character-based estimation if tiktoken is unavailable
    """

    def __init__(
        self,
        llm_client: LLMClient,
        token_limit: int = 120000,  # Default for claude-3-5-sonnet (200k context)
        enable_summarization: bool = True,
        summarize_after_rounds: int = 2,  # è¶…è¿‡ N è½®åè§¦å‘å‹ç¼©
    ):
        """Initialize Token Manager.

        Args:
            llm_client: LLM client for generating summaries
            token_limit: Maximum tokens before triggering summarization
            enable_summarization: Whether to enable automatic summarization
            summarize_after_rounds: Number of rounds after which to trigger compression
        """
        self.llm = llm_client
        self.token_limit = token_limit
        self.enable_summarization = enable_summarization
        self.summarize_after_rounds = summarize_after_rounds
        
        # æ ¸å¿ƒè®°å¿†å­˜å‚¨ï¼ˆè·¨è½®æ¬¡ä¿æŒï¼‰
        self.core_memory: str = ""

        # Initialize tiktoken encoder
        try:
            self.encoding = tiktoken.get_encoding("cl100k_base")
            self.tiktoken_available = True
        except Exception:
            self.encoding = None
            self.tiktoken_available = False

    def estimate_tokens(self, messages: list[Message]) -> int:
        """Accurately calculate token count for message history using tiktoken.

        Uses cl100k_base encoder (GPT-4/Claude/MiniMax compatible).
        Falls back to character-based estimation if tiktoken is unavailable.

        Args:
            messages: List of messages to count tokens for

        Returns:
            Estimated token count
        """
        if not self.tiktoken_available:
            return self._estimate_tokens_fallback(messages)

        total_tokens = 0

        for msg in messages:
            # Count text content
            if isinstance(msg.content, str):
                total_tokens += len(self.encoding.encode(msg.content))
            elif isinstance(msg.content, list):
                for block in msg.content:
                    if isinstance(block, dict):
                        # Convert dict to string for calculation
                        total_tokens += len(self.encoding.encode(str(block)))

            # Count thinking (if present)
            if msg.thinking:
                total_tokens += len(self.encoding.encode(msg.thinking))

            # Count tool_calls (if present)
            if msg.tool_calls:
                total_tokens += len(self.encoding.encode(str(msg.tool_calls)))

            # Metadata overhead per message (approximately 4 tokens)
            total_tokens += 4

        return total_tokens

    def _estimate_tokens_fallback(self, messages: list[Message]) -> int:
        """Fallback token estimation method (when tiktoken is unavailable).

        Uses character-based estimation: ~2.5 characters = 1 token

        Args:
            messages: List of messages to count tokens for

        Returns:
            Estimated token count
        """
        total_chars = 0
        for msg in messages:
            if isinstance(msg.content, str):
                total_chars += len(msg.content)
            elif isinstance(msg.content, list):
                for block in msg.content:
                    if isinstance(block, dict):
                        total_chars += len(str(block))

            if msg.thinking:
                total_chars += len(msg.thinking)

            if msg.tool_calls:
                total_chars += len(str(msg.tool_calls))

        # Rough estimation: average 2.5 characters = 1 token
        return int(total_chars / 2.5)

    async def maybe_summarize_messages(self, messages: list[Message]) -> list[Message]:
        """Summarize message history based on rounds or token limit.

        è§¦å‘æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³è§¦å‘ï¼‰ï¼š
        1. å¯¹è¯è½®æ¬¡è¶…è¿‡ summarize_after_roundsï¼ˆé»˜è®¤ 2 è½®ï¼‰
        2. Token æ•°é‡è¶…è¿‡ token_limit

        ç­–ç•¥ï¼š
        - å‹ç¼©æ—©æœŸè½®æ¬¡ï¼Œæå–æ ¸å¿ƒè®°å¿†
        - ä¿ç•™æœ€è¿‘ 1 è½®çš„å®Œæ•´å¯¹è¯
        - æ ¸å¿ƒè®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’

        Args:
            messages: Current message history

        Returns:
            Summarized message history (or original if no summarization needed)
        """
        if not self.enable_summarization:
            return messages

        # ç»Ÿè®¡å¯¹è¯è½®æ¬¡ï¼ˆuser æ¶ˆæ¯æ•°é‡ï¼Œæ’é™¤ systemï¼‰
        user_indices = [i for i, msg in enumerate(messages) if msg.role == "user" and i > 0]
        num_rounds = len(user_indices)
        estimated_tokens = self.estimate_tokens(messages)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©ï¼šè½®æ¬¡è¶…è¿‡é˜ˆå€¼ æˆ– token è¶…é™
        need_compress = (
            num_rounds > self.summarize_after_rounds or 
            estimated_tokens > self.token_limit
        )

        if not need_compress:
            return messages

        print(f"\nğŸ“Š å¯¹è¯è½®æ¬¡: {num_rounds}, Token: {estimated_tokens}/{self.token_limit}")
        print("ğŸ”„ è§¦å‘è®°å¿†å‹ç¼©...")

        # è‡³å°‘éœ€è¦ 2 è½®æ‰èƒ½å‹ç¼©
        if num_rounds < 2:
            return messages

        # å‹ç¼©ç­–ç•¥ï¼šä¿ç•™æœ€è¿‘ 1 è½®å®Œæ•´å¯¹è¯ï¼Œå‹ç¼©ä¹‹å‰çš„è½®æ¬¡ä¸ºæ ¸å¿ƒè®°å¿†
        rounds_to_compress = num_rounds - 1  # å‹ç¼©é™¤æœ€åä¸€è½®å¤–çš„æ‰€æœ‰è½®æ¬¡
        
        # æ”¶é›†éœ€è¦å‹ç¼©çš„æ¶ˆæ¯
        compress_end_idx = user_indices[-1]  # æœ€åä¸€ä¸ª user æ¶ˆæ¯ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
        messages_to_compress = messages[1:compress_end_idx]  # æ’é™¤ system prompt
        
        if not messages_to_compress:
            return messages

        # ç”Ÿæˆæ ¸å¿ƒè®°å¿†
        core_memory = await self._extract_core_memory(messages_to_compress, rounds_to_compress)
        
        if core_memory:
            self.core_memory = core_memory  # ä¿å­˜æ ¸å¿ƒè®°å¿†
        
        # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
        new_messages = [messages[0]]  # system prompt
        
        # æ³¨å…¥æ ¸å¿ƒè®°å¿†
        if self.core_memory:
            memory_message = Message(
                role="user",
                content=f"[å¯¹è¯å†å²æ ¸å¿ƒè®°å¿†]\n{self.core_memory}\n\nè¯·åŸºäºä»¥ä¸Šå†å²ä¸Šä¸‹æ–‡ç»§ç»­å¯¹è¯ã€‚",
            )
            new_messages.append(memory_message)
            # æ·»åŠ ä¸€ä¸ªç¡®è®¤æ¶ˆæ¯
            new_messages.append(Message(
                role="assistant",
                content="å¥½çš„ï¼Œæˆ‘å·²äº†è§£ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·ç»§ç»­ã€‚",
            ))
        
        # æ·»åŠ æœ€è¿‘ä¸€è½®çš„å®Œæ•´å¯¹è¯
        new_messages.extend(messages[compress_end_idx:])

        new_tokens = self.estimate_tokens(new_messages)
        print(f"âœ“ è®°å¿†å‹ç¼©å®Œæˆ: {estimated_tokens} â†’ {new_tokens} tokens")
        print(f"  å‹ç¼©äº† {rounds_to_compress} è½®å¯¹è¯ï¼Œä¿ç•™æœ€è¿‘ 1 è½®")

        return new_messages
    
    async def _extract_core_memory(self, messages: list[Message], num_rounds: int) -> str:
        """ä»å†å²æ¶ˆæ¯ä¸­æå–æ ¸å¿ƒè®°å¿†.

        Args:
            messages: éœ€è¦å‹ç¼©çš„æ¶ˆæ¯åˆ—è¡¨
            num_rounds: è½®æ¬¡æ•°é‡

        Returns:
            æ ¸å¿ƒè®°å¿†æ–‡æœ¬
        """
        # æ„å»ºå¯¹è¯å†…å®¹
        conversation_text = ""
        for msg in messages:
            if msg.role == "user":
                conversation_text += f"ç”¨æˆ·: {msg.content}\n"
            elif msg.role == "assistant":
                content = msg.content if isinstance(msg.content, str) else str(msg.content)
                # æˆªæ–­è¿‡é•¿å†…å®¹
                if len(content) > 500:
                    content = content[:500] + "..."
                conversation_text += f"åŠ©æ‰‹: {content}\n"
                if msg.tool_calls:
                    tool_names = [tc.function.name for tc in msg.tool_calls]
                    conversation_text += f"  [è°ƒç”¨å·¥å…·: {', '.join(tool_names)}]\n"
            elif msg.role == "tool":
                result = msg.content if isinstance(msg.content, str) else str(msg.content)
                if len(result) > 200:
                    result = result[:200] + "..."
                conversation_text += f"  [å·¥å…·ç»“æœ: {result}]\n"

        # è°ƒç”¨ LLM æå–æ ¸å¿ƒè®°å¿†
        try:
            extract_prompt = f"""è¯·ä»ä»¥ä¸‹ {num_rounds} è½®å¯¹è¯ä¸­æå–æ ¸å¿ƒè®°å¿†ï¼Œç”¨äºåç»­å¯¹è¯çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚

<å¯¹è¯å†å²>
{conversation_text}
</å¯¹è¯å†å²>

è¯·æå–å¹¶æ•´ç†ï¼š
1. **ç”¨æˆ·æ„å›¾**: ç”¨æˆ·æƒ³è¦å®Œæˆä»€ä¹ˆä»»åŠ¡ï¼Ÿ
2. **å…³é”®ä¿¡æ¯**: æåˆ°çš„é‡è¦äº‹å®ã€æ•°æ®ã€æ–‡ä»¶åã€ä½ç½®ç­‰
3. **å·²å®Œæˆæ“ä½œ**: åŠ©æ‰‹å·²ç»åšäº†ä»€ä¹ˆï¼Ÿ
4. **å¾…å¤„ç†äº‹é¡¹**: è¿˜æœ‰ä»€ä¹ˆæ²¡å®Œæˆï¼Ÿ

è¦æ±‚ï¼š
- ç®€æ´æ˜äº†ï¼Œæ§åˆ¶åœ¨ 300 å­—ä»¥å†…
- åªä¿ç•™å¯¹åç»­å¯¹è¯æœ‰ç”¨çš„ä¿¡æ¯
- ä½¿ç”¨ä¸­æ–‡"""

            response = await self.llm.generate(
                messages=[
                    Message(role="system", content="ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ€»ç»“å’Œæå–å…³é”®ä¿¡æ¯çš„åŠ©æ‰‹ã€‚"),
                    Message(role="user", content=extract_prompt),
                ]
            )

            return response.content if response.content else ""

        except Exception as e:
            print(f"âš ï¸ æ ¸å¿ƒè®°å¿†æå–å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç®€å•æ‘˜è¦
            return f"[{num_rounds} è½®å¯¹è¯å†å²ï¼Œæå–å¤±è´¥]"

